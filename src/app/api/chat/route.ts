import { HfInference } from '@huggingface/inference';

const hf = new HfInference(process.env.HF_TOKEN!);

export async function POST(req: Request) {
  const { prompt, role = "default" } = await req.json();

  // D√©finir un prompt syst√®me selon le r√¥le
  let systemPrompt = "Tu es une intelligence artificielle bienveillante.";

  switch (role) {
    case "coach":
      systemPrompt = `
Tu es Sophie, une coach holistique douce et chaleureuse.
Tu aides avec bienveillance et des conseils pratiques sur la vie, le bien-√™tre, les √©motions, les blocages.
Sois directe, concise, mais empathique. Ne reformule pas les questions.
`;
      break;

    case "guide":
      systemPrompt = `
Tu es un guide spirituel inspir√© et po√©tique.
Tu parles avec douceur, profondeur, et sagesse symbolique.
R√©ponds comme un ma√Ætre spirituel ou un chamane bienveillant.
Sois lumineux et clair, sans reformuler la question.
`;
      break;
  }

  const fullPrompt = `${systemPrompt.trim()}

Utilisateur : ${prompt.trim()}
R√©ponse :`;

  try {
    const result = await hf.textGeneration({
      model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',
      inputs: fullPrompt,
      parameters: {
        max_new_tokens: 300,
        temperature: 0.7,
        top_p: 0.95,
        stop: ["Utilisateur :", "Question :", "###"]
      }
    });

    const cleaned = result.generated_text
      .replace(fullPrompt, "")
      .replace(/^["\n\s]+/, "")
      .trim();

    return new Response(JSON.stringify({ response: cleaned }), {
      headers: { "Content-Type": "application/json" }
    });
  } catch (error) {
    console.error("Erreur API HuggingFace :", error);
    return new Response(
      JSON.stringify({ error: "Erreur IA üò¢" }),
      { status: 500, headers: { "Content-Type": "application/json" } }
    );
  }
}
